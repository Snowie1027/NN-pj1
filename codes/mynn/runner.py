import numpy as np
import os
import csv
import pickle
from tqdm import tqdm
from mynn.op import compute_softmax

# å®šä¹‰æ¨¡åž‹è®­ç»ƒå™¨ RunnerM ç±»
class RunnerM():
    def __init__(self, model, optimizer, metric, loss_fn, batch_size, scheduler):
        """
        åˆå§‹åŒ– RunnerM å¯¹è±¡
        :param model: éœ€è¦è®­ç»ƒçš„æ¨¡åž‹
        :param optimizer: ä¼˜åŒ–å™¨å¯¹è±¡(å¦‚ SGD)
        :param metric: è¯„ä¼°æŒ‡æ ‡å¯¹è±¡(å¦‚ Accuracy)
        :param loss_fn: æŸå¤±å‡½æ•°å¯¹è±¡
        :param batch_size: æ¯ä¸ª mini-batch çš„æ ·æœ¬æ•°
        :param scheduler: å­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.metric = metric  # ç¡®ä¿æ˜¯å·²å®žä¾‹åŒ–çš„è¯„ä¼°æŒ‡æ ‡
        self.scheduler = scheduler
        self.batch_size = batch_size

        # è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•çš„æŒ‡æ ‡
        self.train_scores = []  # æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒé›†å¾—åˆ†ï¼ˆå‡†ç¡®çŽ‡ï¼‰
        self.dev_scores = []    # æ¯æ¬¡è¿­ä»£çš„éªŒè¯é›†å¾—åˆ†ï¼ˆå‡†ç¡®çŽ‡ï¼‰
        self.train_loss = []    # æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒé›†æŸå¤±
        self.dev_loss = []      # æ¯æ¬¡è¿­ä»£çš„éªŒè¯é›†æŸå¤±

        # ç”¨äºŽå­˜å‚¨è®­ç»ƒæ—¥å¿—åˆ° CSV æ–‡ä»¶
        self.log_records = []

    def train(self, train_set, dev_set, **kwargs):
        
        num_epochs = kwargs.get("num_epochs", 0)  # é»˜è®¤è®­ç»ƒè½®æ•°ä¸º 0
        log_iters = kwargs.get("log_iters", 100)  # æ—¥å¿—æ‰“å°é—´éš”ï¼ˆç›®å‰æ²¡å¯ç”¨ï¼‰
        save_dir = kwargs.get("save_dir", "best_model")  # æ¨¡åž‹ä¿å­˜ç›®å½•

        # å¦‚æžœä¿å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)

        best_score = 0  # åˆå§‹åŒ–æœ€ä½³éªŒè¯é›†åˆ†æ•°
        X_train, y_train = train_set
        total_iterations = int(np.ceil(X_train.shape[0] / self.batch_size))  # è®¡ç®—æ¯ä¸ª epoch çš„è¿­ä»£æ¬¡æ•°

        # è®­ç»ƒå¾ªçŽ¯
        for epoch in range(num_epochs):
            X, y = train_set
            assert X.shape[0] == y.shape[0]  # ç¡®ä¿è¾“å…¥å’Œæ ‡ç­¾çš„æ ·æœ¬æ•°ä¸€è‡´

            # æ¯ä¸ª epoch å¼€å§‹å‰ï¼Œæ‰“ä¹±è®­ç»ƒæ•°æ®é¡ºåºï¼ˆæå‡æ³›åŒ–èƒ½åŠ›ï¼‰
            idx = np.random.permutation(range(X.shape[0]))
            X = X[idx]
            y = y[idx]

            dev_loss = 0.0
            dev_score = 0.0
            epoch_loss = 0.0
            epoch_score = 0.0
            
            # å¦‚æžœä½¿ç”¨äº†å­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼Œåˆ™æ›´æ–°å­¦ä¹ çŽ‡
            if self.scheduler is not None:
                self.scheduler.step()

            # tqdm ç”¨äºŽæ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
            with tqdm(total=total_iterations, desc=f"Epoch {epoch + 1}/{num_epochs}") as pbar:
                for iteration in range(total_iterations):
                    # èŽ·å–å½“å‰ batch çš„è®­ç»ƒæ•°æ®
                    train_X = X[iteration * self.batch_size: (iteration + 1) * self.batch_size]
                    train_y = y[iteration * self.batch_size: (iteration + 1) * self.batch_size]

                    # è°ƒæ•´è¾“å…¥ç»´åº¦ï¼Œé€‚é…æ¨¡åž‹è¾“å…¥è¦æ±‚ï¼ˆbatch_size, channel, height, widthï¼‰
                    train_X = train_X.reshape(-1, 1, 28, 28)

                    # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ¨¡åž‹è¾“å‡º logits
                    logits = self.model(train_X)

                    trn_loss = self.loss_fn(logits, train_y)
                    epoch_loss += trn_loss
                    
                    # Q4: Implement the cross entropy loss.
                    probs = compute_softmax(logits)
                    trn_score = self.metric(probs, train_y)
                    epoch_score += trn_score

                    # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
                    self.loss_fn.backward()
                    self.optimizer.step()

                    # æ¸…ç©ºæ¨¡åž‹ä¸­çš„æ¢¯åº¦ç¼“å­˜
                    self.model.clear_grad()

                    # æ›´æ–°è¿›åº¦æ¡ä¸­çš„æ˜¾ç¤ºä¿¡æ¯
                    pbar.set_postfix({
                        "train_loss": f"{trn_loss:.4f}",
                        "train_score": f"{trn_score:.4f}"
                    })
                    pbar.update(1)
                    
            
            # è®°å½•æ¯ä¸ª epoch çš„å¹³å‡è®­ç»ƒæŸå¤±ä¸Žå‡†ç¡®çŽ‡
            avg_loss = epoch_loss / total_iterations
            avg_score = epoch_score / total_iterations
            self.train_loss.append(avg_loss)
            self.train_scores.append(avg_score)

            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°å½“å‰æ¨¡åž‹è¡¨çŽ°
            dev_score, dev_loss = self.evaluate(dev_set)
            self.dev_scores.append(dev_score)
            self.dev_loss.append(dev_loss)

            # è®°å½•å½“å‰è®­ç»ƒè¿‡ç¨‹çš„æ—¥å¿—ä¿¡æ¯
            log = {
                "epoch": epoch + 1,
                "iteration": iteration,
                "train_loss": float(trn_loss),
                "train_score": float(trn_score),
                "dev_loss": float(dev_loss),
                "dev_score": float(dev_score)
            }
            self.log_records.append(log)
            if trn_score > best_score:
                save_path = os.path.join(save_dir, 'model.pickle')
                self.save_model(save_path)  # ä¿å­˜å½“å‰æœ€ä½³æ¨¡åž‹
                tqdm.write(f"ðŸŽ‰ Best accuracy updated: {best_score:.5f} --> {trn_score:.5f}")
                best_score = trn_score  # æ›´æ–°æœ€ä½³å¾—åˆ†

        self.best_score = best_score  # ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³åˆ†æ•°
        
    def evaluate(self, data_set):
        X, y = data_set
        X = X.reshape(-1, 1, 28, 28)  # è°ƒæ•´è¾“å…¥å½¢çŠ¶

        logits = self.model(X)  # å‰å‘ä¼ æ’­
        loss = self.loss_fn(logits, y)  # è®¡ç®—æŸå¤±

        probs = compute_softmax(logits)  # è®¡ç®—æ¦‚çŽ‡åˆ†å¸ƒ
        score = self.metric(probs, y)  # è®¡ç®—å‡†ç¡®çŽ‡

        return score, loss

    def save_model(self, save_path):
        self.model.save_model(save_path)

    def save_log_to_csv(self, file_path):
        if not self.log_records:
            print("No logs to save.")
            return

        keys = self.log_records[0].keys()  # æå–æ—¥å¿—ä¸­çš„å­—æ®µåä½œä¸º CSV è¡¨å¤´
        with open(file_path, mode='w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()  # å†™å…¥è¡¨å¤´
            writer.writerows(self.log_records)  # å†™å…¥æ‰€æœ‰æ—¥å¿—è®°å½•


class RunnerM_CNN():
    def __init__(self, model, train_images, train_labels, test_images, test_labels, best_model_path):
        self.model = model
        self.train_images = train_images
        self.train_labels = train_labels
        self.test_images = test_images
        self.test_labels = test_labels
        self.best_model_path = best_model_path

        self.train_scores = []  # æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒé›†å¾—åˆ†ï¼ˆå‡†ç¡®çŽ‡ï¼‰
        self.dev_scores = []    # æ¯æ¬¡è¿­ä»£çš„éªŒè¯é›†å¾—åˆ†ï¼ˆå‡†ç¡®çŽ‡ï¼‰
        self.train_loss = []    # æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒé›†æŸå¤±
        self.dev_loss = []      # æ¯æ¬¡è¿­ä»£çš„éªŒè¯é›†æŸå¤±

        # ç”¨äºŽå­˜å‚¨è®­ç»ƒæ—¥å¿—åˆ° CSV æ–‡ä»¶
        self.log_records = []

    def train(self, image, label, lr):
        """å•å¼ å›¾åƒè®­ç»ƒ"""
        out, loss, acc = self.model.forward(image, label)
        self.model.backward(label, out, lr)
        return loss, acc

    def run_test(self):
        """è¯„ä¼°æ¨¡åž‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨çŽ°"""
        total_loss = 0
        total_correct = 0

        for im, label in zip(self.test_images, self.test_labels):
            _, loss, acc = self.model.forward(im, label)
            total_loss += loss
            total_correct += acc

        val_loss = total_loss / len(self.test_images)
        val_acc = total_correct / len(self.test_images)
        return val_loss, val_acc

    def save_model(self):
        folder = os.path.dirname(self.best_model_path)
        if not os.path.exists(folder):
            os.makedirs(folder)
        with open(self.best_model_path, 'wb') as f:
            pickle.dump(self.model, f)

    def run_train(self, epochs, lr, batch_size):
        num_batches = int(np.ceil(len(self.train_images) / batch_size))
        previous_acc = 0.0  # ä¸Šä¸€ä¸ª epoch çš„è®­ç»ƒå‡†ç¡®çŽ‡

        for epoch in range(epochs):
            permutation = np.random.permutation(len(self.train_images))
            self.train_images = self.train_images[permutation]
            self.train_labels = self.train_labels[permutation]

            total_loss = 0
            total_correct = 0
            
            with tqdm(total=num_batches, desc=f"Epoch {epoch + 1}/{epochs}") as pbar:
                for i, (im, label) in enumerate(zip(self.train_images, self.train_labels)):
                    loss, correct = self.train(im, label, lr=lr)
                    total_loss += loss
                    total_correct += correct

                    avg_loss = total_loss / (i + 1)
                    avg_acc = total_correct / (i + 1)

                    pbar.set_postfix({
                        'train_loss': f'{avg_loss:.4f}',
                        'train_scores': f'{avg_acc:.4f}'
                    })
                    pbar.update(1)

            # æ¯ä¸ª epoch çš„è®­ç»ƒç»“æžœ
            self.train_loss.append(avg_loss)
            self.train_scores.append(avg_acc)

            # ã€æ–°å¢žã€‘è·‘ä¸€ééªŒè¯é›†ï¼Œå¹¶è®°å½•
            val_loss, val_acc = self.run_test()
            self.dev_loss.append(val_loss)
            self.dev_scores.append(val_acc)

            log = {
                "epoch": epoch + 1,
                "train_loss": float(avg_loss),
                "train_scores": float(avg_acc),
                "val_loss": float(val_loss),
                "val_scores": float(val_acc)
            }
            self.log_records.append(log)

            # æ¯”è¾ƒå¹¶è¾“å‡ºç²¾ç®€ä¿¡æ¯
            if avg_acc > previous_acc:
                print(f"âœ… Training accuracy improved: {previous_acc:.4f} --> {avg_acc:.4f}")
                previous_acc = avg_acc
                self.save_model()
